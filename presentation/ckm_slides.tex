%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{amsmath} % Math env.
\usepackage{fontawesome} % Icons for GitHub, etc.

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Statistische Geheimhaltung]{Statistische Geheimhaltung - Cell Key Methode} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Joshua Simon} % Your name
\institute[University Bamberg] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Otto-Friedrich-University Bamberg \\ % Your institution for the title page
\medskip
\textit{joshua-guenter.simon@stud.uni-bamberg.de} % Your email address
}
\date{May 24, 2022} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}


%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section{Einführung} 
%------------------------------------------------

\subsection{Veröffentlichungen in der amtlichen Statistik}

\begin{frame}{}
    \frametitle{Linearly separable data classes}
    First, let's consider a given data set $\mathcal{X}$ of labeled points (inputs) with individual labels $y_i \in \left\{ -1,1 \right\}$, e.g. $(x_1,y_1), ..., (x_m, y_m) \in \mathcal{X} \times \left\{ -1,1 \right\}$. \\~\\
    
    Our goal is to implement a classification method, which is able to classify new and unlabeld data points with the right or 'best' label. \\~\\
\end{frame}


\begin{frame}{}
    \frametitle{Linearly separable data classes}
    In machine learning, a well established classification method are the so called \textbf{Support Vector Machines} (SVM). Developed by Vladimir Vapnik and his coworkers in the 1990s, SVMs are still a relevent topic and an even more powerful tool for \textbf{classification} and \textbf{regression}.
\end{frame}


%------------------------------------------------
\subsection{Warum ist Geheimhaltung notwendig?}

\begin{frame}{}
    \frametitle{Hyperplane classifiers}
    The underlying learning algorithm of SVMs yields to find a hyperplane in some dot product space $\mathcal{H}$, which separates the data. A hyperplane of the form
    \begin{equation}
        \langle w,x \rangle + b = 0
    \end{equation}
    where $w \in \mathcal{H}, b \in \mathbb{R}$ shall be considered \cite{Schoelkopf} (p. 11). Futhermore decision functions 
    \begin{equation}
        f(x) = sgn \left( \langle w,x \rangle + b \right)
    \end{equation}
    can be assigned.
\end{frame}


\begin{frame}{}
    \frametitle{Hyperplane classifiers - A constrained optimization problem}
    The \textbf{optimal hyperplane} can be calculated by finding the normal vector $w$ that leads to the largest margin. Thus we need to solve the optimization problem
    \begin{equation} \label{eq:1}
        \begin{aligned}
            \min_{w \in \mathcal{H}, b \in \mathbb{R}} \quad & \tau (w) = \frac{1}{2} \lVert w \rVert^2 \\
            \textrm{subject to} \quad & y_{i} \left( \langle w,x \rangle + b \right) \geq 1 \text{ } \forall i = {1, \dots, m}. 
        \end{aligned}
    \end{equation}
    The constraints in \eqref{eq:1} ensure that $f(x_i)$ will be $+1$ for $y_i = +1$ and $-1$ for  $y_i = -1$. The $\geq 1$ on the right hand side of the constraints effectively fixes the scaling of $w$. This leads to the maximum margin hyperplane. A detailed explanation can be found in \cite{Schoelkopf}(Chap 7).
\end{frame}


%------------------------------------------------
\section{Etablierte Geheimhaltungsverfahren}
%------------------------------------------------

\subsection{Posttabulare Verfahren}

\begin{frame}{}
	\frametitle{The kernel trick}
	To extend the introduced SVM algorithm, we can substitute \eqref{eq:5} by applying a kernel of the form
    \begin{equation}
        k(x,x') = \langle \Phi (x), \Phi (x') \rangle
    \end{equation}
    where 
    \begin{equation}
        \begin{aligned}
            \Phi: \mathcal{X} & \rightarrow \mathcal{H} \\
            (x) & \mapsto \Phi (x)
        \end{aligned}
    \end{equation}
    is a function that maps an input from $ \mathcal{X} $ into a dot product space $ \mathcal{H} $. This is referred to as the \textbf{kernel trick}.
\end{frame}


%------------------------------------------------
\subsection{Pretabulare Verfahren}

\begin{frame}{}
	\frametitle{A suitable kernel}
	Going back to our problem of non linearly separable data, we can use a kernel function of the form
    \begin{equation}
        k(x, x') = \exp \left( - \frac{\left\lVert x - x' \right\rVert^2}{2 \sigma^2} \right),
    \end{equation}
    a so called \textbf{Gaussian radial basis function} (GRBF or RBF kernels) with $ \sigma > 0$.
\end{frame}



%------------------------------------------------
\section{Cell Key Methode}
%------------------------------------------------

\begin{frame}{}
	\frametitle{More kernel applications}
    Some interessting kernel applications:
    \begin{itemize}
        \item Image recognition/classification (with SVMs) for example in 
            \begin{itemize}
                \item Handwriting recognition
                \item Tumor detection
            \end{itemize}
        \item Computer vision and computer graphics, 3D reconstruction
        \item Kernel principal component analysis
    \end{itemize}
\end{frame}


%------------------------------------------------
\subsection{Methodik}

%------------------------------------------------
\subsection{Beispiel Implementierung}

%------------------------------------------------
\subsection{Anwendung in der Hochschulstatistik}


%------------------------------------------------
\section{Fazit}
%------------------------------------------------


%------------------------------------------------
% References
%------------------------------------------------

\begin{frame}
    \frametitle{References}
    \footnotesize{
        \begin{thebibliography}{99} % Beamer does not support BibTeX so references must be inserted manually as below
            \bibitem[Schölkopf, 2002]{Schoelkopf} Schölkopf, Bernhard, Alexander J. Smola
            \newblock Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT press, 2002.

            \bibitem[Liesen, 2015]{Liesen} Liesen, Jörg, Volker Mehrmann
            \newblock Lineare Algebra. Wiesbaden, Germany: Springer, 2015.

            \bibitem[Jarre, 2019]{Jarre} Jarre, Florian, Josef Stoer
            \newblock Optimierung: Einführung in mathematische Theorie und Methoden. Springer-Verlag, 2019.

            \bibitem[Reinhardt, 2012]{Reinhardt} Reinhardt, Rüdiger, Armin Hoffmann, Tobias Gerlach
            \newblock Nichtlineare Optimierung: Theorie, Numerik und Experimente. Springer-Verlag, 2012.

            \bibitem[Bronstein, 2020]{Bronstein}  Bronstein, Ilja N., et al. 
            \newblock Taschenbuch der Mathematik. 11. Auflage, Springer-Verlag, 2020.

            \bibitem[Chang, 2011]{libsvm} Chang, Chih-Chung, Chih-Jen Lin
            \newblock LIBSVM : A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, 2011. Software available at \url{https://www.csie.ntu.edu.tw/~cjlin/libsvm/}.

            % Example entry.
            %\bibitem[Smith, 2012]{p3} John Smith (2002)
            %\newblock Title of the publication
            %\newblock \emph{Journal Name} 12(3), 45 -- 678.
        \end{thebibliography}
    }
\end{frame}

%------------------------------------------------

\begin{frame}
    \Huge{\centerline{Time for your questions!}}
    \bigskip
    \bigskip
    \bigskip
    \bigskip

    \normalsize
    \centering
    Follow our development on GitHub \faicon{github} 
    \url{https://github.com/JoshuaSimon/Cell-Key-Method}
    
\end{frame}

%----------------------------------------------------------------------------------------

\end{document} 